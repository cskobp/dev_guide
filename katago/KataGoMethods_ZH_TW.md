# KataGo 實現的其他方法 (Other Methods Implemented in KataGo)

此頁面記錄了 KataGo 中實現的一些額外方法和技術，包括未在 [論文](https://arxiv.org/abs/1902.10565) 中提及、後來發明或實現、或從其他研究中借鑑但未在別處記錄的內容。

## 通過 Masking 在多種棋盤尺寸上訓練

KataGo 是第一個能夠使用同一個神經網路在多種棋盤尺寸（從 9x9 到 19x19）上進行超人類水準對弈的圍棋程式。此方法包含兩個部分：
1.  **網路設計**：卷積層和池化層本質上是與尺寸無關的。輸出頭（Output Head）需要特殊設計，例如使用全局平均池化 (Global Average Pooling) 或混合卷積。
2.  **Masking 技巧**：在訓練時將不同尺寸的棋盤混合在同一個 batch 中。將張量設置為最大尺寸，並對較小的輸入進行零填充 (zero-pad)。使用掩碼 (Mask) 區分真實空間和填充空間，並在卷積等操作後重新應用掩碼。

## Fixup 初始化 (已棄用)

在 2020 年 `g170` 運行中，KataGo 使用 Fixup Initialization 取代了 Batch Normalization，成功進行了訓練。該方法通過將 Batch Norm 層替換為簡單的偏置層和縮放層，並調整權重初始化來實現。
**注意：此方法已不再使用，請參閱下方的「固定方差初始化和單一 Batch Norm」。**

## Shaped Dirichlet Noise

為了幫助探索意想不到的好棋，KataGo 嘗試「塑造」Dirichlet 噪聲。不僅僅是均勻分佈噪聲，KataGo 會將一半的噪聲集中在那些策略先驗 (Policy Prior) 雖然低但仍顯著高於大多數其他合法移動的「盲點」移動上。這增加了發現被忽略的關鍵移動的機會。

## 根節點策略 Softmax 溫度

為了穩定早期運行並改善探索，在應用 Dirichlet 噪聲之前，KataGo 對根節點的策略先驗應用略大於 1 的 Softmax 溫度（例如 1.25 逐漸衰減至 1.1）。這相當於將所有 logit 向 0 縮放，迫使 MCTS 搜索積極地對抗這種力量以保持策略的銳利度，從而防止在證據不足的情況下過早收斂到某個移動。

## 策略驚喜加權 (Policy Surprise Weighting)

KataGo 在訓練數據中會增加那些策略目標相對於策略先驗「非常令人驚訝」的樣本的權重。具體來說，對於全搜索 (full-searched) 的移動，KataGo 根據 KL 散度重新分配頻率權重，使得「令人驚訝」的局面在訓練數據中被記錄的頻率更高。這有助於網路更快地學習糾正其錯誤判斷。

## 子樹價值偏差修正 (Subtree Value Bias Correction)

這是一種啟發式方法，旨在提高 MCTS 搜索評估的準確性。它不直接估計效用值，而是專注於在線學習神經網路在某些局部模式下的**偏差**。
*   將節點根據最後移動的模式（玩家、位置、周圍 5x5 若有、劫爭等）分桶。
*   計算每個桶的加權平均觀測偏差 (ObsBias)。
*   在計算節點效用 (NodeUtility) 時，減去該桶的偏差值來修正原始神經網路的評估。
這有助於在搜索樹的其他部分遇到相同戰術模式時，利用之前的搜索結果進行修正。

## 動態方差縮放 cPUCT (Dynamic Variance-Scaled cPUCT)

KataGo 根據效用值的經驗方差動態調整 cPUCT係數。
*   如果 playouts 的效用變化很大，應該增加探索（增加 cPUCT），以免過早利用第一個高值移動。
*   如果效用非常穩定，應該減少探索，專注於區分細微差別。
此方法結合不確定性加權，顯著提高了棋力（約 75 Elo）。

## 短期價值和分數目標 (Short-term Value and Score Targets)

KataGo 訓練神經網路預測多個輔助價值目標，這些目標是未來 MCTS 值的指數平均（例如未來 6 回合、16 回合、50 回合）。這有助於提供低方差的反饋，並支持下述的不確定性估計方法。

## 不確定性加權 MCTS Playouts (Uncertainty-Weighted MCTS Playouts)

KataGo 訓練網路預測其當前輸出與未來 MCTS 值之間的短期誤差（不確定性）。
*   在 MCTS 中，主要根據神經網路報告的不確定性來降低那些誤差可能較大的 playouts 的權重。
*   神經網路對 playout 的估計越不確定，該 playout 在計算平均值和驅動 PUCT 時的權重就越小（視為「分數次」playout）。

## 嵌套瓶頸殘差網路 (Nested Bottleneck Residual Nets)

受到 Gumbel MuZero 論文的啟發，KataGo 採用了一種改進的瓶頸殘差塊結構。
*   相比於標準的瓶頸塊（1x1 -> 3x3 -> 1x1），使用更長的瓶頸塊（1x1 -> 3x3 -> 3x3 -> ... -> 1x1）可以更有效地分攤 1x1 卷積的成本。
*   KataGo 進一步將內部的 3x3 卷積配對成嵌套的殘差塊，以改善優化。
這導致了更高效的網路架構 (如 `b18c384`)。

## 輔助軟策略目標 (Auxiliary Soft Policy Target)

添加一個輔助策略頭，嘗試預測以 (1/T) 冪次處理的策略目標（軟化後的策略）。這迫使網路不僅要預測最好的 1-2 個移動，還要正確預測次優移動的相對權重，從而形成更好的內部特徵，加速學習。

## 固定方差初始化和單一 Batch Norm

這是取代 Fixup 初始化的新方法。
1.  **固定方差初始化**：將所有 Batch Norm 層替換為純量乘法，該常數 K 的設定旨在將理想化的方差重置為 1。這適用於更複雜的架構。
2.  **單一 Batch Norm**：
    *   在主幹 (Trunk) 的末端添加**一個** Batch Norm 層。
    *   大部分損失權重 (80%) 通過這個 BN 層。
    *   複製一組不通過 BN 層的頭 (Heads)，給予較小權重 (20%)，這些是用於推理的頭。
這保留了 BN 的優化優勢，同時避免了推理時的麻煩。

## 樂觀策略 (Optimistic Policy)

訓練一個額外的策略頭，該頭專注於預測那些「短期結果比預期好得多」的樣本。這相當於訓練一個「樂觀」的策略。
在 MCTS 測試時，雙方都使用這個樂觀策略。這似乎有助於發現神經網路原始評估忽略的戰術（盲點），或者在劣勢下找到對手最難應對的複雜局面，而不是過早放棄。

---
*文件生成時間: 2026-01-22*
